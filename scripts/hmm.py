import numpy as np
from sklearn.utils import check_array
from sklearn.utils.validation import check_is_fitted
from sklearn import cluster
from sklearn.mixture import gmm

class GaussianHMM:
    """Hidden Markov Model with Gaussian emissions.
       Created as an exercise and based off of the
       models from the hmmlearn library.

    Parameters
    ----------
    n_components: int
        the number of states (hidden) in the hmm

    startprob_prior: array, shape (n_components, )
        prior distribution (state probabilities)

    transmat_prior: array, shape(n_components, n_components)
        prior transition matrix

    n_iter: int
        maximum number of iterations to perform

    tolerance: float
        tolerance for EM (EM will stop when the log-likelihood
        is within the tolerance)

    verbose: bool
        whether to print convergence reports to stderr

    params: string
        controls which parameters are updated when training
        's': startprob
        't': transmat
        'm': means
        'c': covariances

        defaults to all

    init_params: string
        controls which parameters are already supplied for
        training
        's': prob_prior
        't': transmat_prior
        'm': means_prior
        'c': covars_prior

        if a parameter is unsupplied, it defaults to a uniform prior

    Attributes
    ----------
    startprob_: array, shape (n_components, )
        initial state distribution

    transmat_: array, shape (n_components, n_components)
        transition matrix
    """
    def __init__(self, n_components=1,
                 startprob_prior=1.0, transmat_prior=1.0,
                 means_prior=0, means_weight=0,
                 covars_prior=0.01, covars_weight=1,
                 min_covar=0.001, covariance_type='diag',
                 n_iter=10, tolerance=0.01, verbose=False,
                 params = 'stmc', init_params=''):
        self.n_components = n_components
        self.startprob_prior = startprob_prior
        self.transmat_prior = transmat_prior
        self.means_prior = means_prior
        self.means_weight = means_weight
        self.covars_prior = covars_prior
        self.covars_weight = covars_weight
        self.min_covar = min_covar
        self.covariance_type = covariance_type
        self.n_iter = n_iter
        self.tolerance = tolerance
        self.verbose = verbose
        self.params = params
        self.init_params = init_params
    
    def fit(self, X, lengths=None):
        """Infer the model parameters
           (i.e., initial distribution and transition matrix)
           using EM algorithm

        Parameters
        ----------
        X: array-like, shape(n_samples, n_features)
            feature matrix --- each row is a feature vector
            corresponding to a sample

        lengths: array-like of ints, shape(n_sequences, )
            lengths of the sequences in X; each sequence
            is an enumeration of related samples
        """
        X = check_array(X)
        self._init_params(X, lengths)
        self._check()

        for _ in range(self.n_iter):
            # start[i] = P(first sample generated by ith state)
            # trans = transition matrix
            logprob = 0
            for start, end in _gen_sequences(X, lengths):
                loglikelihood = self._compute_log_likelihood(X[start, end])
                # TODO

    def score(self, X, lengths=None):
        """Returns the log probability of X under the model

        Parameters
        ----------
        X: array-like, shape(n_samples, n_features)
            feature matrix --- each row is a feature vector
            corresponding to a sample
 
        lengths: array-like of ints, shape(n_sequences, )
            lengths of the sequences in X; each sequence
            is an enumeration of related samples
        """
        check_is_fitted(self, 'startprob_')
        self._check()
        X = check_array(X)

        logprob = 0
        # TODO

    def _viterbi_pass(self, logprob):
        # TODO

    def _forward_pass(self, logprob):
        # TODO

    def _backwardpass(self, logprob):
        # TODO

    def _compute_posteriors(self, fdwlattice, bwdlattice):
        # TODO
    
    def _compute_log_likelihood(self, X):
        """Compute the log likelihood of feature matrix X"""
        return gmm.log_multivariate_normal_density(X, self.means_, self.covars_,
                                                   self.covariance_type)

    def _init_M_stats(self):
        """Initialize dict of stats for M step of EM algorithm"""
        stats = {'n_samples': 0,
                'start': np.zeros(self.n_components),
                'trans': np.zeros((self.n_components, self.n_components)),
                'post': np.zeros(self.n_components),
                'obs': np.zeros((self.n_components, self.n_components)),
                'obs**2': np.zeros((self.n_components, self.n_features))}
        if self.covariance_type == 'full':
            stats['obs*obs.T'] = np.zeros((self.n_components, self.n_features,
                                          self.n_features))
        return stats

    def _accumulate_M_stats(self, stats, X, logprob, posteriors,
                            fwdlattice, bwdlattice):
        # TODO

    def _m_step(self, stats):
        # TODO

    def _init_params(self, X, lengths):
        """Initialize parameters for training"""
        self.n_features = X.shape[1]
        uniform = 1.0 / self.n_components
        # start distribution
        if 's' in self.init_params:
            self.startprob_ = self.startprob_prior
        else:
            self.startprob_ = np.full(self.n_components, uniform)

        # transition matrix
        if 't' in self.init_params:
            self.transmat_ = self.transmat_prior
        else:
            self.transmat_ = np.full((self.n_samples, self.n_components), uniform)

        # means
        if 'm' in self.init_parms:
            self.means_ = self.means_prior
        else:
            kmeans = cluster.KMeans(n_clusters=n_components)
            kmeans.fit(X)
            self.means_ = kmeans.cluster_centers_

        # covariances
        if 'c' in self.init_params:
            self.covars_ = self.covars_prior
        else:
            cv = np.cov(X.T) + self.min_covar * np.eye(X.shape[1])
            if self.covariance_type == 'diag':
                self.covars_ = np.tile(np.diag(cv), (self.n_components, 1))
            else: # full covariance matrix
                self.covars_ = np.tile(cv, (n_components, 1, 1))

    def _check(self):
        """Sanity check for model parameters

        Raises
        ------
        ValueError
            if parameters (startprob, transmat) are invalid

        """
        if self.startprob_.shape[0] != self.n_components:
            raise ValueError("startprob_ length doesn't match n_components")
        if not np.allclose(self.startprob_.sum(), 1.0):
            raise ValueError("startprob_ doesn't sum to 1.0")

        if self.transmat_.shape != (self.n_components, self.n_components):
            raise ValueError("transmat_ is not size (n_components x n_components)")
        if not np.allclose(self.transmat_.sum(axis=1), 1.0):
            raise ValueError("rows of transmat_ don't sum to 1.0")

    def _gen_sequences(X, lengths):
        """Generator for sequence bounds in X"""
        if lengths is None:
            yield 0, len(X)
        else:
            ends = np.cumsum(lengths)
            starts = ends - lengths
            if ends[-1] > X.shape[0]:
                raise ValueError("lengths describes {0} samples and X \
                                  describes {1} samples".format(X.shape[0],
                                                                ends[-1]))
            for i in range(len(lengths)):
                yield starts[i], ends[i]


